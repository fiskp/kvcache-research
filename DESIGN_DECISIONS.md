# Milestone 1 — DHT Protocol Comparison: Design Decisions & Implementation Plan

## 1. Project Context

P2P-RAGCache aims to reduce LLM inference latency (specifically Time-to-First-Token) by
distributing precomputed KV caches across a decentralized peer-to-peer overlay network.
Milestone 1 lays the foundation: **select and build the DHT overlay** that all subsequent
milestones depend on.

### Requirements derived from the spec

| Requirement | Source |
|---|---|
| Organise 10–50 inference nodes into a self-organising DHT | M1 objective |
| O(log N) lookup performance | M1 task 4 |
| Graceful node join / leave | M1 task 3 |
| Leaf Set replication in Milestone 2 | M2 task 2 |
| Parallel multi-source fetching in Milestone 3 | M3 objective |
| TTFT reduction of 50–70% vs disk baseline | Section 6.2 |
| Linear throughput scaling up to 20 nodes | Section 6.2 |
| Cache hit rate > 85% | Section 6.2 |
| Python 3.10+ with PyTorch 2.0+ software stack | Section 3.1 |

---

## 2. Protocol Selection Criteria

The spec names three candidates: **Chord**, **Pastry**, and **Kademlia**.  We evaluated all
three equally against the criteria below, weighted by relevance to the P2P-RAGCache use
case.

| Criterion | Weight | Why it matters |
|---|---|---|
| **Routing correctness** | Critical | Maps to cache hit rate (>85% target). An incorrect lookup means a cache miss and a full recompute. |
| **Lookup latency (hop count)** | High | Directly impacts TTFT. Fewer hops = lower overlay latency before the KV cache transfer begins. |
| **Scalability (5–50 nodes)** | High | The system targets a moderate cluster, not internet-scale. Must perform well at N=10–50. |
| **Join / leave cost** | Medium | Inference nodes may restart or scale up/down. Expensive joins waste GPU time. |
| **Parallel lookup support** | Medium | M3 requires fetching cache chunks from multiple peers simultaneously. Protocols with built-in parallel query patterns have an advantage. |
| **State overhead** | Low | At 10–50 nodes, memory for routing tables is negligible compared to KV cache storage. |
| **Implementation complexity** | Medium | This is a research prototype with a single developer. Simpler = faster to correct. |

---

## 3. Implementation Decisions

### 3.1 Language: Python

**Decision:** Python 3.10+

**Rationale:**
- The rest of the P2P-RAGCache stack (PyTorch, CUDA kernels, RAG pipeline) is Python.
  Keeping the DHT in Python avoids FFI overhead and simplifies the integration path for
  Milestones 2–5.
- At 10–50 nodes the DHT overlay is not the bottleneck — network I/O and GPU transfer
  are.  Python's performance is sufficient for routing logic.
- Rapid prototyping matters more than micro-optimisation at the research stage.

**Trade-off acknowledged:** Go would give better concurrency primitives (goroutines) and
lower per-hop latency.  If profiling in later milestones shows the DHT layer is a bottleneck,
the routing core could be rewritten in Go or Rust behind a Python wrapper.

### 3.2 Simulation approach: in-process, no real sockets

**Decision:** All nodes run as Python objects inside a single process.  A `NetworkSimulator`
class acts as a registry; cross-node communication is a direct method call.

**Rationale:**
- Eliminates port management, firewall, and serialisation complexity during the comparison
  phase.
- Lets us benchmark **protocol-level** differences (hop count, correctness, state overhead)
  without conflating them with OS-level networking noise.
- The user will add real multi-node networking later; the `DHTNode` abstract base class
  defines the interface that a networked implementation will honour.

**Assumption:** Simulated hop counts and correctness results are representative of real
deployment because the routing algorithms are identical — only the transport changes.

### 3.3 ID space: 16-bit (configurable)

**Decision:** Default `id_bits = 16` (65 536 possible IDs).

**Rationale:**
- Large enough to avoid collisions with 50 nodes but small enough for human-readable
  debugging and visualisation.
- All implementations accept `id_bits` as a constructor parameter, so switching to 128-bit
  or 160-bit IDs for production is a one-line change.

### 3.4 Deterministic ID / key generation

**Decision:** Node IDs and test keys are generated by hashing `"node-{seed}-{i}"` /
`"key-{seed}-{i}"` with SHA-1, then taking the result modulo 2^id_bits.

**Rationale:**
- Produces uniformly distributed IDs (important for fair ring partitioning).
- Deterministic by seed, so every benchmark run is exactly reproducible.

---

## 4. Per-Protocol Design Notes

### 4.1 Chord

**Implementation:** `dht_comparison/chord.py`

- **Ring + finger table.**  Each node stores `predecessor`, `successor`, and a finger table
  of `id_bits` entries where `finger[i] = successor(node_id + 2^i)`.
- **Lookup:** Iterative.  The initiating node walks the ring by repeatedly asking the closest
  preceding finger for the next hop.  We track hop count in the `LookupResult`.
- **Join:** The joining node asks the bootstrap to look up each finger target (O(log^2 N)
  messages).  Registration happens *after* the finger table is built so the incomplete node
  doesn't corrupt in-flight lookups.
- **Stabilisation:** Each round does:
  1. Check successor's predecessor (the core Chord stabilise step).
  2. Notify successor.
  3. Fix **all** fingers via local lookups (trades per-round cost for faster convergence).
  4. Maintain a successor list of length 3 for fault tolerance.
- **Ground truth:** `successor(key)` = first node with ID >= key, wrapping.

### 4.2 Kademlia

**Implementation:** `dht_comparison/kademlia.py`

- **XOR distance + k-buckets.**  Bucket `i` holds up to `k` nodes at XOR distance in
  `[2^i, 2^(i+1))`.  Default `k = 8`, `alpha = 3`.
- **Lookup:** Iterative with parallelism parameter alpha.  Each round queries up to alpha
  unqueried nodes from the shortlist.  Rounds continue until no new closer nodes are
  discovered.  A "hop" is one round (even though it contacts alpha nodes in parallel).
  This means Kademlia's hop count understates its total message count but fairly represents
  its *latency* (parallel queries happen concurrently in a real network).
- **find_node_rpc:** The responding node includes itself in the returned candidate list
  (consistent with real implementations).
- **Join:** Register, seed one bucket entry from the bootstrap, then do a self-lookup to
  populate buckets.  Very cheap (~2–4 messages).
- **Stabilisation:** Remove dead nodes from all buckets.
- **Ground truth:** Node with smallest XOR distance to the key.

**Design choice — k and alpha at small N:**  With only 10–50 nodes, standard Kademlia
defaults (k=20, alpha=3) would mean every node knows nearly every other node, collapsing
the routing to 1 hop.  We use k=8 to keep the routing table realistically sparse so hop-count
scaling is visible.

### 4.3 Pastry

**Implementation:** `dht_comparison/pastry.py`

- **Prefix routing table + leaf set.**  With `b = 4` (base-16 digits) and 16-bit IDs, the
  routing table is 4 rows x 16 columns.  The leaf set holds the 8 numerically closest nodes.
- **Routing:** Three-phase:
  1. Prefix match: find an entry in `routing_table[prefix_len][key_digit]` that extends the
     shared prefix by one digit.
  2. Leaf set / routing table fallback: if no prefix entry exists, route to the numerically
     closest known node.
  3. Terminate when no known node is closer than the current node.
- **Join:** Copy the bootstrap's full state, do a self-lookup, learn from every node on the
  path, then announce to leaf-set neighbours.
- **Stabilisation:** Prune dead entries; exchange state with leaf neighbours.
- **Ground truth:** Numerically closest node on the ring (circular distance).

---

## 5. Assumptions

| # | Assumption | Impact if wrong |
|---|---|---|
| A1 | **Hop count is a valid proxy for lookup latency.** In a real network each hop adds one RTT. | If hops are fast but bandwidth is the bottleneck, hop count overstates Chord/Kademlia's disadvantage vs Pastry. |
| A2 | **Uniform key distribution.** Test keys are SHA-1 hashes, uniformly spread. | Real KV cache keys may cluster (e.g., keys for the same document are numerically adjacent). Clustering could benefit Pastry's leaf set but penalise Chord if many keys fall between two adjacent nodes. |
| A3 | **Moderate churn.** Nodes join/leave gracefully, not crash. | Crash failures require faster failure detection (timeouts, heartbeats) which we have not benchmarked. Kademlia is known to be more resilient to crash failures because its bucket refresh mechanism is lazy. |
| A4 | **In-process simulation is faithful.** No real serialisation, no packet loss, no congestion. | Protocols that generate more messages (Chord's finger-fix) would be penalised more in a real network than in simulation. |
| A5 | **10–50 nodes is the target scale.** We do not optimise for thousands of nodes. | At 10–50 nodes, all three protocols are fast; the differences are small in absolute terms. At 1000+ nodes, Pastry's per-hop prefix resolution gives a stronger advantage. |
| A6 | **The "responsible node" definition differs per protocol** (successor, XOR-closest, numerically closest). | This is intentional — each protocol's correctness is measured against its own ground truth. The choice of responsibility rule will affect cache placement in M2. |

---

## 6. Benchmark Results Summary

All tests run with `id_bits = 16`, deterministic seeds, and sufficient stabilisation rounds.

### 6.1 Lookup hop count (N = 20, 200 random keys)

| Protocol | Mean | Median | P95 | Max | Correctness |
|---|---|---|---|---|---|
| Chord | 1.90 | 2.0 | 4 | 4 | **100%** |
| Kademlia | 1.61 | 1.0 | 3 | 3 | **100%** |
| Pastry | **1.12** | 1.0 | 2 | 2 | 91.5% |

### 6.2 Scalability (hop count vs N)

| N | Chord | Kademlia | Pastry |
|---|---|---|---|
| 5 | 1.18 | 1.00 | 0.83 |
| 10 | 1.53 | 1.04 | 1.01 |
| 20 | 1.90 | 1.61 | 1.12 |
| 50 | 2.76 | 2.50 | 1.44 |

All three exhibit sub-linear (logarithmic) growth.  Pastry grows slowest because each hop
resolves 4 bits (b=4) instead of ~1 bit.

### 6.3 Join cost & state overhead

| Protocol | Join msgs (mean) | Routing table entries (N=20) |
|---|---|---|
| Chord | 16.0 | 4.8 |
| Kademlia | **2.4** | 12.5 |
| Pastry | 7.0 | 19.9 |

### 6.4 Churn resilience (3/20 nodes removed gracefully)

| Protocol | Post-churn correctness |
|---|---|
| Chord | **100%** |
| Kademlia | **100%** |
| Pastry | 91% |

### 6.5 Throughput (in-process, N=20, 1000 lookups)

| Protocol | Lookups/sec |
|---|---|
| Chord | **~157K** |
| Pastry | ~81K |
| Kademlia | ~21K |

Kademlia's lower throughput is an artefact of the iterative multi-query design.  In a real
network with parallel I/O, each "round" contacts alpha nodes concurrently, so wall-clock
latency per lookup would be similar to Chord's — the extra messages happen in parallel.

---

## 7. Analysis & Recommendation

### Chord

**Strengths:** Perfect correctness at all scales; highest throughput; simplest mental model
(ring + fingers); smallest routing state; matches the spec's terminology ("ring," "finger
tables," "successor").

**Weaknesses:** Most expensive joins (O(log^2 N) messages to build the full finger table);
no built-in notion of parallel queries.

**Fit for P2P-RAGCache:** Strong baseline.  Correctness is the most important property for
a cache lookup overlay — an incorrect route means a cache miss and a costly recompute.

### Kademlia

**Strengths:** Perfect correctness; cheapest joins; parallel iterative lookups map directly
onto M3's multi-source fetching; mature ecosystem (used by BitTorrent, Ethereum, IPFS/libp2p).

**Weaknesses:** Highest message count per lookup (queries alpha nodes per round); slightly
more complex to reason about (XOR metric is less intuitive than ring distance).

**Fit for P2P-RAGCache:** Strong contender.  The parallel lookup is a natural fit for
Milestone 3, and the XOR metric distributes load more evenly than ring-based approaches.

### Pastry

**Strengths:** Fewest hops per lookup (prefix routing resolves b=4 bits per hop); leaf set
concept aligns with M2's "Leaf Set replication."

**Weaknesses:** Correctness degrades as N grows (79% at N=50) because the routing table
is sparse for a 16-bit ID space with b=4; most complex implementation; worst churn
resilience in our tests.

**Fit for P2P-RAGCache:** The correctness gap is a concern.  It could be mitigated with
larger leaf sets, more aggressive stabilisation, or a smaller ID space, but this adds
complexity.  The "Leaf Set" concept from M2 can be implemented on top of any protocol
(Chord's successor list and Kademlia's k-closest serve the same role).

### Recommendation

**Primary choice: Chord** — for its perfect correctness, simplicity, and alignment with the
spec's language.  It is the safest foundation for a research prototype where debugging time
is expensive.

**Strong alternative: Kademlia** — if Milestone 3's parallel fetching proves critical to
TTFT reduction, Kademlia's iterative parallel lookup provides a cleaner integration path.
Consider switching to Kademlia after M1 if early profiling shows that lookup parallelism
dominates the TTFT budget.

**Not recommended at this time: Pastry** — the correctness gap and implementation
complexity outweigh its hop-count advantage at the project's target scale (10–50 nodes).

---

## 8. Forward-Looking: Impact on Milestones 2–5

| Milestone | How the DHT choice affects it |
|---|---|
| **M2 — Cache Distribution** | "Leaf Set replication" can be implemented as Chord's successor list (replicate to k successors) or Kademlia's k-closest nodes.  No protocol lock-in. |
| **M3 — Parallel Fetching** | Kademlia has a natural advantage (alpha-parallel lookups).  With Chord, parallel fetching would be built *above* the DHT layer (query multiple replica holders in parallel), which is straightforward. |
| **M4 — Queue-Aware Prefetching** | Protocol-agnostic; depends on the inference queue, not the DHT. |
| **M5 — RoPE Cache Stitching** | Protocol-agnostic; operates on retrieved KV tensors regardless of how they were located. |

---

## 9. How to Reproduce

```bash
# Install
pip install -r requirements.txt

# Run unit tests (27 tests, ~2 seconds)
pytest tests/ -v -s

# Run full benchmark report
python benchmark.py
```

All results are deterministic (seeded ID/key generation).  The same numbers will reproduce
on any machine.
